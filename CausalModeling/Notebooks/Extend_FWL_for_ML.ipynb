{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extend FWL for Machine Learning\n",
        "\n",
        "So where does Machine Learning (ML) come into the picture?\n",
        "\n",
        "There are three main cases where we might want to use other ML algorithms instead of OLS\n",
        "1. When there are significant non-linearities present\n",
        "2. When the number of confounders you're trying to control for is too large (m > n)\n",
        "3. When you're trying to extract Heterogenous Treatment Effect (HTE)\n",
        "\n",
        "If you're attempted to think, why should we even use FWL? can't we just run the ML algorithm as is and treat this as a prediction task?\n",
        "\n",
        "The answer is resounding **NO**. Inference and prediction are two completely different tasks. Prediction cares about the accuracy of the prediction of $y$ where inference cares about the accuracy of the coefficient $\\beta$."
      ],
      "metadata": {
        "id": "6rDuzYME64Pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So how can we use ML for inference?\n",
        "\n",
        "Lets go back to the FWL decomposition:\n",
        "*   regress $x_1$ on $x_2$ to $x_n$ and retrieve the residuals $\\tilde x_1$\n",
        "*   regress $y$ on $x_2$ to $x_n$ and retrieve the residuals $\\tilde y$\n",
        "*   regress $\\tilde y$ on $\\tilde x_1$ and retrieve the coefficient on $\\tilde x_1$ <br>\n",
        "<br>\n",
        "\n",
        "The first 2 steps are actually prediction tasks, where it turns out we can use any ML algorithm, but there are 2 things we need to watch out for.\n",
        "\n",
        "1. **Overfitting**. Many ML algorithms tend to overfit the data, and when we overfit, we end up including parts of the data which is actually unrelated to the regressor. In the worst case it would drive the residuals to zero.\n",
        "2. **Regularization Bias**. Bias generated from how many ML algorithms conduct variable selection implicitly through penalization\n",
        "\n"
      ],
      "metadata": {
        "id": "3IreQfN1_KCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "P_2m8QGB_JT1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def generate_data(seed=1, N=300, K=5):\n",
        "\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # visit history\n",
        "    visitHistory = np.round(np.random.normal(10,5,N),0)\n",
        "    visitHistory[visitHistory<0] = 0\n",
        "\n",
        "    # trx history\n",
        "    trxHistory = np.round(.2*visitHistory * np.random.normal(5,2,N),0)\n",
        "    trxHistory[trxHistory<0] = 0\n",
        "\n",
        "    # SkuAvailability in terms of oos pct\n",
        "    skuAvailability = np.random.normal(0.7,0.1,N)\n",
        "\n",
        "    # number of sales visit\n",
        "    salesVisit = np.round(visitHistory * 0.2 + trxHistory * 0.1 +\n",
        "                          np.random.normal(1,1, N), 0)\n",
        "    salesVisit[salesVisit<0] = 0\n",
        "\n",
        "    # Sales\n",
        "    sales = np.round(5000 * salesVisit + 20000 * visitHistory +\n",
        "                     10000 * trxHistory + 200000 * skuAvailability +\n",
        "                     np.random.normal(20000,5000,N),0)\n",
        "\n",
        "    # Generate the dataframe\n",
        "    df = pd.DataFrame({'visitHistory': visitHistory,\n",
        "                       'trxHistory': trxHistory,\n",
        "                       'skuAvailability': skuAvailability,\n",
        "                       'salesVisit': salesVisit,\n",
        "                       'sales': sales})\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "AC1cPLDPxTcf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YfMqx5B6xarm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create our sample splitting \"object\"\n",
        "kf = KFold(n_splits=5,shuffle=True,random_state=42)\n",
        "\n",
        "# apply the splits to our Xs\n",
        "kf.get_n_splits(X)\n",
        "\n",
        "# initialize columns for residuals\n",
        "yresid = y*0\n",
        "dresid = d*0\n",
        "\n",
        "# Now loop through each fold\n",
        "ii=0\n",
        "for train_index, test_index in kf.split(X):\n",
        "  X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
        "  y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "  d_train, d_test = d.iloc[train_index,:], d.iloc[test_index,:]\n",
        "\n",
        "  # Do DML thing\n",
        "  # Ridge y on training folds:\n",
        "  ridgey.fit(X_train, y_train)\n",
        "\n",
        "  # but get residuals in test set\n",
        "  yresid.iloc[test_index]=y_test-ridgey.predict(X_test)\n",
        "\n",
        "  #Ridge d on training folds\n",
        "  ridged.fit(X_train, d_train)\n",
        "\n",
        "  #but get residuals in test set\n",
        "  dresid.iloc[test_index,:]=d_test-ridged.predict(X_test)\n",
        "\n",
        "\n",
        "# Regress resids\n",
        "dmlreg=linear_model.LinearRegression().fit(dresid,yresid)\n",
        "\n",
        "print(\"DML regression earnings race gap: {:.3f}\".format(dmlreg.coef_[0]))"
      ],
      "metadata": {
        "id": "Ul-rCf0KwoZf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}