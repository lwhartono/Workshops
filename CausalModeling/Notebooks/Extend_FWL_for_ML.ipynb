{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6rDuzYME64Pk"
      },
      "source": [
        "# Extend FWL for Machine Learning\n",
        "\n",
        "So where does Machine Learning (ML) come into the picture?\n",
        "\n",
        "There are three main cases where we might want to use other ML algorithms instead of OLS\n",
        "1. When there are significant non-linearities present\n",
        "2. When the number of confounders you're trying to control for is too large (m > n)\n",
        "3. When you're trying to extract Heterogenous Treatment Effect (HTE)\n",
        "\n",
        "If you're attempted to think, why should we even use FWL? can't we just run the ML algorithm as is and treat this as a prediction task?\n",
        "\n",
        "The answer is resounding **NO**. Inference and prediction are two completely different tasks. Prediction cares about the accuracy of the prediction of $y$ where inference cares about the accuracy of the coefficient $\\beta$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3IreQfN1_KCq"
      },
      "source": [
        "So how can we use ML for inference?\n",
        "\n",
        "Lets go back to the FWL decomposition:\n",
        "*   regress $x_1$ on $x_2$ to $x_n$ and retrieve the residuals $\\tilde x_1$\n",
        "*   regress $y$ on $x_2$ to $x_n$ and retrieve the residuals $\\tilde y$\n",
        "*   regress $\\tilde y$ on $\\tilde x_1$ and retrieve the coefficient on $\\tilde x_1$ <br>\n",
        "<br>\n",
        "\n",
        "The first 2 steps are actually prediction tasks, where it turns out we can use any ML algorithm, but there are 2 things we need to watch out for.\n",
        "\n",
        "1. **Overfitting**. Many ML algorithms tend to overfit the data, and when we overfit, we end up including parts of the data which is actually unrelated to the regressor. In the worst case it would drive the residuals to zero.\n",
        "2. **Regularization Bias**. Bias generated from how many ML algorithms conduct variable selection implicitly through penalization\n",
        "\n",
        "The above 2 can issues can be solved with a technique similar to cross validation\n",
        "\n",
        "We'll rename $x_2$ to $x_n$ as W in the examples below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "cellView": "form",
        "id": "P_2m8QGB_JT1"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "AC1cPLDPxTcf"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def generate_data(seed=1, N=300, K=5):\n",
        "\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # visit history\n",
        "    visitHistory = np.round(np.random.normal(10,5,N),0)\n",
        "    visitHistory[visitHistory<0] = 0\n",
        "\n",
        "    # trx history\n",
        "    trxHistory = np.round(.2*visitHistory * np.random.normal(5,2,N),0)\n",
        "    trxHistory[trxHistory<0] = 0\n",
        "\n",
        "    # SkuAvailability in terms of oos pct\n",
        "    skuAvailability = np.random.normal(0.7,0.1,N)\n",
        "\n",
        "    # number of sales visit\n",
        "    salesVisit = np.round(visitHistory * 0.2 + trxHistory * 0.1 +\n",
        "                          np.random.normal(1,1, N), 0)\n",
        "    salesVisit[salesVisit<0] = 0\n",
        "\n",
        "    # Sales\n",
        "    sales = np.round(5000 * salesVisit + 20000 * visitHistory +\n",
        "                     10000 * trxHistory + 200000 * skuAvailability +\n",
        "                     np.random.normal(20000,5000,N),0)\n",
        "\n",
        "    # Generate the dataframe\n",
        "    df = pd.DataFrame({'visitHistory': visitHistory,\n",
        "                       'trxHistory': trxHistory,\n",
        "                       'skuAvailability': skuAvailability,\n",
        "                       'salesVisit': salesVisit,\n",
        "                       'sales': sales})\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "cellView": "form",
        "id": "YfMqx5B6xarm"
      },
      "outputs": [],
      "source": [
        "#@generate-data\n",
        "df = generate_data(seed = 6, N=200)\n",
        "y = df['sales']\n",
        "W = df[['visitHistory','trxHistory']]\n",
        "X = df['salesVisit']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mJnHf09WPqNi"
      },
      "source": [
        "Go through the FWL Steps <br>\n",
        "Step 1: predict X using W with ML and compute residuals $\\tilde x$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHWFAgdiQJsj"
      },
      "outputs": [],
      "source": [
        "rfx = RandomForestRegressor(max_depth=4, random_state=0).fit(W, X)\n",
        "xResid=X-rfx.predict(W)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjWZGUu-PzT1"
      },
      "source": [
        "Step 2: predict y using W with ML and compute residuals $\\tilde y$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "mfVwQtQPGBag"
      },
      "outputs": [],
      "source": [
        "rfy = RandomForestRegressor(max_depth=4, random_state=0).fit(W, y)\n",
        "yResid=y-rfy.predict(W)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gMOLykxQQjwH"
      },
      "source": [
        "Step 3: regress $\\tilde y$ on $\\tilde x$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC84Xj_7N1Sp",
        "outputId": "9509a449-b7d0-44eb-c440-9ff70ef8af1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:         salesResiduals   R-squared:                       0.044\n",
            "Model:                            OLS   Adj. R-squared:                  0.039\n",
            "Method:                 Least Squares   F-statistic:                     9.083\n",
            "Date:                Thu, 15 Jun 2023   Prob (F-statistic):            0.00292\n",
            "Time:                        04:14:56   Log-Likelihood:                -2281.9\n",
            "No. Observations:                 200   AIC:                             4568.\n",
            "Df Residuals:                     198   BIC:                             4574.\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=======================================================================================\n",
            "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
            "---------------------------------------------------------------------------------------\n",
            "Intercept             466.1959   1551.002      0.301      0.764   -2592.407    3524.799\n",
            "salesVisitResiduals  5300.3541   1758.668      3.014      0.003    1832.231    8768.477\n",
            "==============================================================================\n",
            "Omnibus:                        3.756   Durbin-Watson:                   2.042\n",
            "Prob(Omnibus):                  0.153   Jarque-Bera (JB):                3.670\n",
            "Skew:                           0.196   Prob(JB):                        0.160\n",
            "Kurtosis:                       3.535   Cond. No.                         1.13\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ],
      "source": [
        "dfRes = pd.DataFrame({'salesVisitResiduals':xResid,\n",
        "                      'salesResiduals':yResid})\n",
        "mod = smf.ols('salesResiduals ~ salesVisitResiduals', data=dfRes)\n",
        "res = mod.fit()\n",
        "print(res.summary())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MNRF3qvsRH9Y"
      },
      "source": [
        "### Try to do it with any ML algo of your choosing below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lzf16-5VLTTZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
